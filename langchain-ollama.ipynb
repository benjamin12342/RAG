{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceBgeEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "llm = ChatOllama(model = \"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "file_path = 'data.csv'\n",
    "texts = []\n",
    "buffer = []\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "with open(file_path, newline='', encoding='UTF-8-sig') as f:\n",
    "     reader = csv.DictReader(f, delimiter=',')\n",
    "     for i, row in enumerate(reader):\n",
    "        \n",
    "          text = f\"{row['상표한글명']}, {row['상표영문명']}, {row['상품류별버전']}, {row['류']}, {row['유사군']}, {row['지정상품한글명']}, {row['유사군']}\"\n",
    "          \n",
    "          buffer.append(text)\n",
    "          if (i + 1) % 20 == 0:\n",
    "               combined_text = \"\\n\".join(buffer)\n",
    "               texts.append(combined_text)\n",
    "               buffer = [] \n",
    "     if buffer:\n",
    "          texts.append(\"\\n\".join(buffer)) \n",
    "# for index, block in enumerate(texts):\n",
    "#     print(f\"Block {index + 1}:\\n{block}\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件路径: /home/wangao/RAG/test.pdf\n"
     ]
    }
   ],
   "source": [
    "file_path = \"test.pdf\"\n",
    "print(\"文件路径:\", os.path.abspath(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start object is not a byte.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring (part of) ToUnicode map because the PDF data does not conform to the format. This could result in (cid) values in the output. The start object is not a byte.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.schema import Document\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = UnstructuredLoader(file_path)\n",
    "docs = loader.load()  \n",
    "page_docs = []\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    for page_number, page in enumerate(pdf.pages):\n",
    "        page_content = page.extract_text() \n",
    "        if page_content: \n",
    "            if len(page_content) > 500:\n",
    "                split_texts = text_splitter.split_text(page_content)\n",
    "                for text in split_texts:\n",
    "                    page_docs.append({\n",
    "                        \"page_number\": page_number + 1,\n",
    "                        \"content\": text\n",
    "                    })\n",
    "            else:\n",
    "               \n",
    "                page_docs.append({\n",
    "                    \"page_number\": page_number + 1,\n",
    "                    \"content\": page_content\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Load pretrained SentenceTransformer: BAAI/bge-m3\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=doc[\"content\"], metadata={\"page_number\": doc[\"page_number\"]})\n",
    "    for doc in page_docs\n",
    "]\n",
    "\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "model_kwargs = {\"device\": \"cuda\"}  \n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"score_threshold\": 0.1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert in determining whether a trademark is suspected of plagiarism and meets qualification standards. Only answer \"Y\" or \"N\" based on the provided trademark law document (`example.pdf`) and the registered data (`data.csv`). Do not provide explanations or any additional information.\n",
    "**Important: Do not provide any other information or explanations. Only output `Y` for available or `N` for not available.**\n",
    "\n",
    "Trademark: {trademark}\n",
    "Availability:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    {\"trademark\": RunnablePassthrough()}  # 传入输入参数\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = \"/home/wangao/RAG/TB_KT10_bulk_testset_26.txt\"\n",
    "input_file_path = \"/home/wangao/RAG/TB_KT10_bulk_testset_26.txt\"\n",
    "output_file_path = \"/home/wangao/RAG/trademark_availability_results.txt\"\n",
    "\n",
    "# Open the input and output files\n",
    "with open(input_file_path, \"r\", encoding=\"utf-8\") as input_file, open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for line in input_file:\n",
    "        trademark = line.strip()  # Remove any extra whitespace/newlines\n",
    "        result = chain.invoke({\"trademark\": trademark})  # Get the model's output\n",
    "        \n",
    "        # Ensure only \"Y\" or \"N\" is written, or default to \"N\"\n",
    "        availability = result if result in [\"Y\", \"N\"] else \"N\"\n",
    "        \n",
    "        # Write the result to the output file\n",
    "        output_file.write(f\" {availability}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "\n",
      "The context contains a series of documents with metadata and page content related to patent law, specifically regarding trademark registration and dispute resolution. In one of the documents (Document(metadata={'page_number': 38}), page_content='...'), there is a section on \"심판청구서 등의 각하에 관한 적용례\" which translates to \"Application examples for dismissal of petitions, etc.\".\n",
      "\n",
      "This suggests that the context indeed contains information related to \"심판청구서 등의 각하에 관한 적용례\".\n"
     ]
    }
   ],
   "source": [
    "reponse2 = chain.invoke(\"\"\"is \"심판청구서 등의 각하에 관한 적용례\" in the list?\"\"\")\n",
    "print(reponse2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.v12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
